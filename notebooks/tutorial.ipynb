{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgriFrost-AI Tutorial: End-to-End Frost Forecasting\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"../docs/logo/AgriFrost-AI-transparent.png\" alt=\"AgriFrost-AI Logo\" width=\"150\"/>\n",
    "\n",
    "## üå°Ô∏è AgriFrost-AI Complete Workflow Demonstration\n",
    "\n",
    "**Complete example from data loading to model training to prediction generation**\n",
    "\n",
    "*F3 Innovate Frost Risk Forecasting Challenge (2025)*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Tutorial Contents\n",
    "\n",
    "This notebook will guide you through the following steps:\n",
    "\n",
    "1. **Environment Setup and Data Loading**\n",
    "2. **Data Exploration and Visualization**\n",
    "3. **Feature Engineering Demonstration**\n",
    "4. **Model Training (LightGBM)**\n",
    "5. **Model Evaluation and Visualization**\n",
    "6. **Generate Predictions**\n",
    "\n",
    "**Estimated Time**: ~30-60 minutes (depending on data size and hardware)\n",
    "\n",
    "**Requirements**:\n",
    "- Python 3.10+\n",
    "- Project dependencies installed (`pip install -r requirements.txt`)\n",
    "- Data downloaded to `data/raw/frost-risk-forecast-challenge/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root directory to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set plotting style (compatible with different matplotlib versions)\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Project root directory: {project_root}\")\n",
    "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "First, let's load the raw data and explore its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load raw data\n",
    "from src.data.loaders import DataLoader\n",
    "\n",
    "data_path = project_root / \"data/raw/frost-risk-forecast-challenge/cimis_all_stations.csv.gz\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"‚ùå Data file not found: {data_path}\")\n",
    "    print(\"Please download the data first (refer to docs/README.md)\")\n",
    "else:\n",
    "    print(f\"üìÇ Loading data: {data_path}\")\n",
    "    loader = DataLoader()\n",
    "    df_raw = loader.load_raw_data(data_path)\n",
    "    print(f\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"   Shape: {df_raw.shape}\")\n",
    "    print(f\"   Columns: {len(df_raw.columns)}\")\n",
    "    print(f\"   Time range: {df_raw['Date'].min()} to {df_raw['Date'].max()}\")\n",
    "    print(f\"   Number of stations: {df_raw['Stn Id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 View data overview\n",
    "if 'df_raw' in locals():\n",
    "    print(\"üìä Data Overview:\")\n",
    "    print(df_raw.head(10))\n",
    "    print(\"\\nüìã Data Information:\")\n",
    "    print(df_raw.info())\n",
    "    print(\"\\nüìà Descriptive Statistics:\")\n",
    "    print(df_raw.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization\n",
    "\n",
    "Let's visualize some key patterns and features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Time series visualization\n",
    "if 'df_raw' in locals():\n",
    "    # Convert Date column to datetime\n",
    "    df_raw['Date'] = pd.to_datetime(df_raw['Date'])\n",
    "    \n",
    "    # Select a single station for visualization (e.g., Station 2)\n",
    "    df_station = df_raw[df_raw['Stn Id'] == 2].copy()\n",
    "    df_station = df_station.sort_values('Date')\n",
    "    \n",
    "    # Take the last 1000 rows for quick visualization\n",
    "    df_sample = df_station.tail(1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Temperature time series\n",
    "    axes[0].plot(df_sample['Date'], df_sample['Air Temp (C)'], label='Air Temperature', linewidth=1)\n",
    "    axes[0].axhline(y=0, color='r', linestyle='--', label='Frost Threshold (0¬∞C)')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Temperature (¬∞C)')\n",
    "    axes[0].set_title('Air Temperature Time Series (Station 2, Last 1000 Hours)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Relative humidity time series\n",
    "    axes[1].plot(df_sample['Date'], df_sample['Rel Hum (%)'], label='Relative Humidity', color='green', linewidth=1)\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Relative Humidity (%)')\n",
    "    axes[1].set_title('Relative Humidity Time Series (Station 2, Last 1000 Hours)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Visualization complete! Showing Station 2's last 1000 hours of data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Frost event statistics\n",
    "if 'df_raw' in locals():\n",
    "    # Identify frost events (‚â§0¬∞C)\n",
    "    df_raw['is_frost'] = (df_raw['Air Temp (C)'] <= 0.0).astype(int)\n",
    "    \n",
    "    # Statistics of frost events by month\n",
    "    df_raw['Month'] = pd.to_datetime(df_raw['Date']).dt.month\n",
    "    frost_by_month = df_raw.groupby('Month')['is_frost'].agg(['sum', 'count', 'mean'])\n",
    "    frost_by_month.columns = ['Frost Events', 'Total Observations', 'Frost Rate']\n",
    "    \n",
    "    print(\"üìä Monthly frost event statistics:\")\n",
    "    print(frost_by_month)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Frost rate by month\n",
    "    axes[0].bar(frost_by_month.index, frost_by_month['Frost Rate'] * 100, color='steelblue')\n",
    "    axes[0].set_xlabel('Month')\n",
    "    axes[0].set_ylabel('Frost Rate (%)')\n",
    "    axes[0].set_title('Frost Rate by Month')\n",
    "    axes[0].set_xticks(range(1, 13))\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Number of frost events by month\n",
    "    axes[1].bar(frost_by_month.index, frost_by_month['Frost Events'], color='coral')\n",
    "    axes[1].set_xlabel('Month')\n",
    "    axes[1].set_ylabel('Number of Frost Events')\n",
    "    axes[1].set_title('Total Frost Events by Month')\n",
    "    axes[1].set_xticks(range(1, 13))\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚ùÑÔ∏è Total frost events: {df_raw['is_frost'].sum():,}\")\n",
    "    print(f\"üìä Frost rate: {df_raw['is_frost'].mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing Pipeline\n",
    "\n",
    "Now let's use the unified data processing pipeline to clean data, generate features, and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Configure data processing pipeline\n",
    "from src.data import DataPipeline\n",
    "\n",
    "# Configuration (using Top 175 features)\n",
    "config = {\n",
    "    \"cleaning\": {\n",
    "        \"config_path\": str(project_root / \"config/data_cleaning.yaml\")\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"threshold\": 0.0  # Frost threshold: 0¬∞C\n",
    "    },\n",
    "    \"feature_engineering\": {\n",
    "        \"enabled\": True,\n",
    "        \"feature_selection\": {\n",
    "            \"method\": \"top_k\",\n",
    "            \"top_k\": 175  # Use Top 175 features\n",
    "        }\n",
    "    },\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuring data pipeline...\")\n",
    "pipeline = DataPipeline(config=config)\n",
    "print(\"‚úÖ Data pipeline created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Process data (using sampling to speed up demonstration)\n",
    "if 'data_path' in locals() and data_path.exists():\n",
    "    print(\"üîÑ Starting data processing...\")\n",
    "    print(\"   ‚ö†Ô∏è  Note: For demonstration speed, we use sampled data (100,000 rows)\")\n",
    "    print(\"   üí° For actual training, remove sample_size parameter to use full data\")\n",
    "    \n",
    "    # Process data (with sampling)\n",
    "    dataset_bundle = pipeline.run(\n",
    "        data_path=data_path,\n",
    "        horizons=[12],  # Only process 12h horizon\n",
    "        use_feature_engineering=True,\n",
    "        sample_size=100000,  # Sample 100,000 rows for demonstration\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    df_processed = dataset_bundle.data\n",
    "    print(f\"‚úÖ Data processing complete!\")\n",
    "    print(f\"   Processed shape: {df_processed.shape}\")\n",
    "    print(f\"   Number of features: {len(dataset_bundle.feature_columns)}\")\n",
    "    print(f\"   Number of labels: {len(dataset_bundle.label_columns)}\")\n",
    "    \n",
    "    # Display feature columns\n",
    "    print(f\"\\nüìã Feature column examples (first 20):\")\n",
    "    for i, feat in enumerate(dataset_bundle.feature_columns[:20]):\n",
    "        print(f\"   {i+1}. {feat}\")\n",
    "    if len(dataset_bundle.feature_columns) > 20:\n",
    "        print(f\"   ... (Total {len(dataset_bundle.feature_columns)} features)\")\n",
    "else:\n",
    "    print(\"‚ùå Data file not found, skipping data processing step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Now let's train a LightGBM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Prepare training data\n",
    "if 'df_processed' in locals():\n",
    "    from src.training.data_preparation import prepare_features_and_targets\n",
    "    from src.evaluation.validators import CrossValidator\n",
    "    from src.models.registry import get_model_class\n",
    "    \n",
    "    # Step 1: Time series split (to avoid data leakage)\n",
    "    # Note: Must split data first, then prepare features and labels\n",
    "    print(\"üìä Performing time series split...\")\n",
    "    train_df, val_df, test_df = CrossValidator.time_split(\n",
    "        df=df_processed,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        date_col=\"Date\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   Training set: {len(train_df)} samples\")\n",
    "    print(f\"   Validation set: {len(val_df)} samples\")\n",
    "    print(f\"   Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Step 2: Prepare features and labels for each split (12h horizon)\n",
    "    print(\"\\nüîß Preparing training set features and labels...\")\n",
    "    X_train, y_frost_train, y_temp_train = prepare_features_and_targets(\n",
    "        df=train_df,\n",
    "        horizon=12,\n",
    "        track=\"top175_features\"\n",
    "    )\n",
    "    \n",
    "    print(\"üîß Preparing validation set features and labels...\")\n",
    "    X_val, y_frost_val, y_temp_val = prepare_features_and_targets(\n",
    "        df=val_df,\n",
    "        horizon=12,\n",
    "        track=\"top175_features\"\n",
    "    )\n",
    "    \n",
    "    print(\"üîß Preparing test set features and labels...\")\n",
    "    X_test, y_frost_test, y_temp_test = prepare_features_and_targets(\n",
    "        df=test_df,\n",
    "        horizon=12,\n",
    "        track=\"top175_features\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Data preparation complete!\")\n",
    "    print(f\"   Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"   Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"   Frost events (training set): {y_frost_train.sum()} ({y_frost_train.mean()*100:.2f}%)\")\n",
    "    print(f\"   Average temperature (training set): {y_temp_train.mean():.2f}¬∞C\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping model training (data not processed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Train classification model (frost probability prediction)\n",
    "if 'X_train' in locals():\n",
    "    print(\"ü§ñ Training frost classification model (LightGBM)...\")\n",
    "    \n",
    "    # Get model class\n",
    "    ModelClass = get_model_class('lightgbm')\n",
    "    \n",
    "    # Create model instance\n",
    "    frost_model = ModelClass(\n",
    "        config={\n",
    "            'task_type': 'classification',\n",
    "            'model_params': {\n",
    "                'n_estimators': 100,  # Fewer trees for demonstration, can use more in practice\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 7,\n",
    "                'random_state': 42,\n",
    "                'verbosity': -1\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    frost_model.fit(\n",
    "        X=X_train,\n",
    "        y=y_frost_train,\n",
    "        eval_set=[(X_val, y_frost_val)]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Classification model training complete!\")\n",
    "    \n",
    "    # Train regression model (temperature prediction)\n",
    "    print(\"ü§ñ Training temperature regression model (LightGBM)...\")\n",
    "    \n",
    "    temp_model = ModelClass(\n",
    "        config={\n",
    "            'task_type': 'regression',\n",
    "            'model_params': {\n",
    "                'n_estimators': 100,\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 7,\n",
    "                'random_state': 42,\n",
    "                'verbosity': -1\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    temp_model.fit(\n",
    "        X=X_train,\n",
    "        y=y_temp_train,\n",
    "        eval_set=[(X_val, y_temp_val)]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Regression model training complete!\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping model training (data not prepared)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Visualization\n",
    "\n",
    "Let's evaluate the model's performance and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Evaluate classification model\n",
    "if 'frost_model' in locals():\n",
    "    from src.evaluation.metrics import MetricsCalculator\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_frost_pred = frost_model.predict(X_test)\n",
    "    y_frost_proba = frost_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    class_metrics = metrics_calc.calculate_classification_metrics(\n",
    "        y_true=y_frost_test,\n",
    "        y_pred=y_frost_pred,\n",
    "        y_proba=y_frost_proba\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Classification Model Performance (Test Set):\")\n",
    "    print(f\"   ROC-AUC: {class_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"   PR-AUC: {class_metrics['pr_auc']:.4f}\")\n",
    "    print(f\"   Brier Score: {class_metrics['brier_score']:.4f}\")\n",
    "    print(f\"   ECE: {class_metrics['ece']:.4f}\")\n",
    "    print(f\"   Accuracy: {class_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {class_metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall: {class_metrics['recall']:.4f}\")\n",
    "    print(f\"   F1 Score: {class_metrics['f1_score']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping classification evaluation (model not trained)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Evaluate regression model\n",
    "if 'temp_model' in locals():\n",
    "    # Generate predictions\n",
    "    y_temp_pred = temp_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    reg_metrics = metrics_calc.calculate_regression_metrics(\n",
    "        y_true=y_temp_test,\n",
    "        y_pred=y_temp_pred\n",
    "    )\n",
    "    \n",
    "    print(\"üìä Regression Model Performance (Test Set):\")\n",
    "    print(f\"   MAE: {reg_metrics['mae']:.4f}¬∞C\")\n",
    "    print(f\"   RMSE: {reg_metrics['rmse']:.4f}¬∞C\")\n",
    "    print(f\"   R¬≤: {reg_metrics['r2']:.4f}\")\n",
    "    print(f\"   MAPE: {reg_metrics.get('mape', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping regression evaluation (model not trained)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Visualize prediction results\n",
    "if 'y_temp_pred' in locals() and 'y_frost_proba' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Temperature prediction vs true values\n",
    "    axes[0, 0].scatter(y_temp_test, y_temp_pred, alpha=0.5, s=10)\n",
    "    axes[0, 0].plot([y_temp_test.min(), y_temp_test.max()], \n",
    "                    [y_temp_test.min(), y_temp_test.max()], \n",
    "                    'r--', lw=2, label='Perfect Prediction')\n",
    "    axes[0, 0].set_xlabel('True Temperature (¬∞C)')\n",
    "    axes[0, 0].set_ylabel('Predicted Temperature (¬∞C)')\n",
    "    axes[0, 0].set_title(f'Temperature Prediction (R¬≤ = {reg_metrics[\"r2\"]:.4f})')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Temperature prediction error distribution\n",
    "    temp_errors = y_temp_pred - y_temp_test\n",
    "    axes[0, 1].hist(temp_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Prediction Error (¬∞C)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title(f'Temperature Prediction Error Distribution (MAE = {reg_metrics[\"mae\"]:.4f}¬∞C)')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. ROC curve\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(y_frost_test, y_frost_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes[1, 0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axes[1, 0].set_xlabel('False Positive Rate')\n",
    "    axes[1, 0].set_ylabel('True Positive Rate')\n",
    "    axes[1, 0].set_title('ROC Curve for Frost Classification')\n",
    "    axes[1, 0].legend(loc=\"lower right\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Frost probability distribution\n",
    "    axes[1, 1].hist(y_frost_proba[y_frost_test == 0], bins=50, alpha=0.7, label='No Frost', color='blue')\n",
    "    axes[1, 1].hist(y_frost_proba[y_frost_test == 1], bins=50, alpha=0.7, label='Frost', color='red')\n",
    "    axes[1, 1].set_xlabel('Predicted Frost Probability')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Frost Probability Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualization complete!\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping visualization (prediction results not generated)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "Let's examine which features are most important to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Get feature importance\n",
    "if 'frost_model' in locals() and 'X_train' in locals():\n",
    "    try:\n",
    "        # Get feature importance (using LightGBM booster API)\n",
    "        # Note: LightGBM requires using booster_.feature_importance() method\n",
    "        feature_importance = frost_model.model.booster_.feature_importance(importance_type='gain')\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_train.columns.tolist(),  # Use training set feature columns\n",
    "            'importance': feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Display Top 20 most important features\n",
    "        print(\"üîù Top 20 Most Important Features:\")\n",
    "        print(importance_df.head(20).to_string(index=False))\n",
    "        \n",
    "        # Visualize Top 20 feature importance\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        top_features = importance_df.head(20)\n",
    "        ax.barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels(top_features['feature'].values)\n",
    "        ax.set_xlabel('Feature Importance (Gain)')\n",
    "        ax.set_title('Top 20 Feature Importance (Frost Classification Model)')\n",
    "        ax.invert_yaxis()  # Most important at top\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Unable to get feature importance: {e}\")\n",
    "        print(\"   This may be because the model type doesn't support it or model is not properly initialized\")\n",
    "        print(\"   Trying to use feature_importances_ attribute:\")\n",
    "        try:\n",
    "            # Fallback method: use feature_importances_ attribute\n",
    "            feature_importance = frost_model.model.feature_importances_\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_train.columns.tolist(),\n",
    "                'importance': feature_importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(\"\\n‚úÖ Successfully obtained feature importance using fallback method:\")\n",
    "            print(importance_df.head(20).to_string(index=False))\n",
    "        except Exception as e2:\n",
    "            print(f\"   Fallback method also failed: {e2}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping feature importance analysis (model not trained)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Predictions\n",
    "\n",
    "Finally, let's use the trained model to generate new predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Prepare new data for prediction\n",
    "if 'X_test' in locals() and 'frost_model' in locals():\n",
    "    # Use a portion of the test set as new data\n",
    "    new_data = X_test[:100].copy()  # Take first 100 samples\n",
    "    \n",
    "    # Generate predictions\n",
    "    frost_proba_predictions = frost_model.predict_proba(new_data)\n",
    "    temp_predictions = temp_model.predict(new_data)\n",
    "    \n",
    "    # Create prediction results DataFrame\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Frost_Probability': frost_proba_predictions,\n",
    "        'Temperature_Prediction_C': temp_predictions,\n",
    "        'Frost_Risk': ['Low' if p < 0.1 else 'Medium' if p < 0.5 else 'High' for p in frost_proba_predictions]\n",
    "    })\n",
    "    \n",
    "    print(\"üìä Prediction Results Example (First 20):\")\n",
    "    print(predictions_df.head(20).to_string(index=True))\n",
    "    \n",
    "    # Statistics of high-risk predictions\n",
    "    high_risk = (predictions_df['Frost_Probability'] > 0.5).sum()\n",
    "    print(f\"\\n‚ö†Ô∏è  High-risk predictions (probability > 0.5): {high_risk} / {len(predictions_df)} ({high_risk/len(predictions_df)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Skipping prediction generation (model or data not available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Visualize prediction results\n",
    "if 'predictions_df' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Frost probability distribution\n",
    "    axes[0].hist(predictions_df['Frost_Probability'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0].axvline(x=0.5, color='r', linestyle='--', linewidth=2, label='High Risk Threshold (0.5)')\n",
    "    axes[0].set_xlabel('Frost Probability')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Frost Probability Predictions')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Temperature prediction distribution\n",
    "    axes[1].hist(predictions_df['Temperature_Prediction_C'], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Frost Threshold (0¬∞C)')\n",
    "    axes[1].set_xlabel('Predicted Temperature (¬∞C)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distribution of Temperature Predictions')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Prediction visualization complete!\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping visualization (prediction results not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "Congratulations! You have completed the complete workflow demonstration of AgriFrost-AI!\n",
    "\n",
    "### üìã Tutorial Summary\n",
    "\n",
    "‚úÖ **Completed**:\n",
    "1. Data loading and exploration\n",
    "2. Data visualization and statistical analysis\n",
    "3. Data cleaning and feature engineering\n",
    "4. Model training (classification and regression)\n",
    "5. Model evaluation and performance analysis\n",
    "6. Feature importance analysis\n",
    "7. Prediction generation and visualization\n",
    "\n",
    "### üöÄ Next Steps Suggestions\n",
    "\n",
    "1. **Try Different Models**:\n",
    "   - XGBoost: `get_model_class('xgboost')`\n",
    "   - CatBoost: `get_model_class('catboost')`\n",
    "   - LSTM: `get_model_class('lstm')` (requires GPU)\n",
    "\n",
    "2. **Try Different Time Horizons**:\n",
    "   - Modify `horizons=[3, 6, 12, 24]` to train multiple horizons\n",
    "\n",
    "3. **Try Different Feature Sets**:\n",
    "   - Full feature set (298 features)\n",
    "   - Custom feature selection\n",
    "\n",
    "4. **Spatial Aggregation**:\n",
    "   - Try Matrix Cell C/D (multi-station features)\n",
    "   - Try Matrix Cell E (graph neural networks)\n",
    "\n",
    "5. **Model Tuning**:\n",
    "   - Use hyperparameter optimization\n",
    "   - Try different model configurations\n",
    "\n",
    "### üìö Further Learning\n",
    "\n",
    "- üìñ **Quick Start**: `docs/README.md`\n",
    "- üèóÔ∏è **Implementation Guide**: `docs/IMPLEMENTATION_GUIDE.md` / `docs/IMPLEMENTATION_GUIDE_CN.md`\n",
    "- üî¨ **Technical Documentation**: `docs/technical/TECHNICAL_DOCUMENTATION.md`\n",
    "- ü§ñ **Model Guide**: `docs/MODELS_GUIDE.md`\n",
    "- üìä **Feature Guide**: `docs/features/FEATURE_GUIDE.md`\n",
    "\n",
    "### üí° Tips\n",
    "\n",
    "- Using the full dataset can achieve better performance (remove `sample_size` parameter)\n",
    "- Increasing `n_estimators` can improve model accuracy (but training time will be longer)\n",
    "- Using GPU can accelerate deep learning model training\n",
    "- LOSO evaluation can test model's spatial generalization capability\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using AgriFrost-AI!** üå°Ô∏èü§ñ\n",
    "\n",
    "**Documentation Version**: 1.0  \n",
    "**Last Updated**: 2025-12-06  \n",
    "**Author**: Zhengkun LI (TRIC Robotics / UF ABE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
