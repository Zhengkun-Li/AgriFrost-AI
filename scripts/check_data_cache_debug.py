#!/usr/bin/env python3
"""Check for data cache and add debugging output.

This script checks:
1. If there are any cached data files that might be reused
2. Adds debugging output to verify neighbor features are generated
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

print("=" * 90)
print("ğŸ” æ•°æ®ç¼“å­˜æ£€æŸ¥å’Œè°ƒè¯•")
print("=" * 90)

# 1. Check for cached data files
print("\n1. æ£€æŸ¥æ•°æ®ç¼“å­˜:")
print("-" * 90)

cache_dirs = [
    project_root / "data" / "processed",
    project_root / "data" / "processed" / "labeled",
    project_root / "data" / "processed" / "pipeline_bundles",
]

found_cache = False
for cache_dir in cache_dirs:
    if cache_dir.exists():
        parquet_files = list(cache_dir.glob("**/*.parquet"))
        if parquet_files:
            found_cache = True
            print(f"\nâœ… æ‰¾åˆ°ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"   Parquetæ–‡ä»¶æ•°é‡: {len(parquet_files)}")
            for pf in parquet_files[:5]:  # Show first 5
                print(f"   - {pf.relative_to(project_root)}")
            if len(parquet_files) > 5:
                print(f"   ... è¿˜æœ‰ {len(parquet_files) - 5} ä¸ªæ–‡ä»¶")

if not found_cache:
    print("\nâœ… æ²¡æœ‰æ‰¾åˆ°æ•°æ®ç¼“å­˜æ–‡ä»¶")

# 2. Check if DataPipeline has caching mechanism
print("\n2. æ£€æŸ¥DataPipelineç¼“å­˜æœºåˆ¶:")
print("-" * 90)

from src.data.pipeline import DataPipeline

# Check if DataPipeline saves intermediate results
pipeline_code = Path("src/data/pipeline.py").read_text()

has_caching = False
if "cache" in pipeline_code.lower() or "save" in pipeline_code.lower():
    print("âš ï¸  DataPipelineä»£ç ä¸­åŒ…å«'cache'æˆ–'save'å…³é”®å­—")
    print("   éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜é€»è¾‘")
    has_caching = True
else:
    print("âœ… DataPipelineä»£ç ä¸­æ²¡æœ‰æ˜æ˜¾çš„ç¼“å­˜æœºåˆ¶")
    print("   æ¯æ¬¡è°ƒç”¨run()éƒ½ä¼šé‡æ–°å¤„ç†æ•°æ®")

# 3. Summary
print("\n" + "=" * 90)
print("ğŸ“‹ æ€»ç»“:")
print("=" * 90)

if found_cache:
    print("âš ï¸  å‘ç°æ•°æ®ç¼“å­˜æ–‡ä»¶:")
    print("   å»ºè®®ï¼šå¦‚æœè¦ç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®ï¼Œå¯ä»¥åˆ é™¤è¿™äº›ç¼“å­˜æ–‡ä»¶")
    print("   å‘½ä»¤ç¤ºä¾‹:")
    for cache_dir in cache_dirs:
        if cache_dir.exists():
            print(f"   rm -rf {cache_dir}")
else:
    print("âœ… æ²¡æœ‰å‘ç°æ•°æ®ç¼“å­˜ï¼Œæ¯æ¬¡è®­ç»ƒéƒ½ä¼šé‡æ–°ç”Ÿæˆæ•°æ®")

print()
print("âœ… å·²æ·»åŠ è°ƒè¯•è¾“å‡ºåˆ°:")
print("   - TrainingRunner.run(): æ£€æŸ¥DataPipelineè¿”å›çš„DataFrame")
print("   - prepare_features_and_targets(): æ£€æŸ¥å„ä¸ªé˜¶æ®µçš„neighborç‰¹å¾")
print()
print("ä¸‹ä¸€æ­¥ï¼šé‡æ–°è¿è¡Œè®­ç»ƒï¼ŒæŸ¥çœ‹è°ƒè¯•è¾“å‡º")


